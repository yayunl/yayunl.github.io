---
layout: post
title:  "Natural Language Processing 0.1.0: Recurrent Neural Networks (RNN)"
date:   2024-08-01 12:00:00
categories: NLP deep-learning
tags: RNN
excerpt: This post discusses the recurrent neural networks.
mathjax: true
use_mermaid: true
---

* content
{:toc}

# Recurrent Neural Networks (RNN)
A typical neural network is a _feedforward network_ which takes in a single large vector and processes in one go. In contrast, a _recurrent neural network_ (RRN)
processes sequences by iterating through the sequence elements one at a time and maintaining a _state_ containing information relative to what it has seen so far.

An RNN is a type of neural network that has an internal loop depicted in the following figure:  

![rnn](/assets/images/NLP/003/rnn.png)  

A lot of time the RRN is illustrated with a sequence of unrolled blocks. The last output at time _t-1_ is fed into the block of time _t_.  

![unrolled-rnn](/assets/images/NLP/003/unrolled-rnn.png)


**Why not use a standard NN to for the sequence processing?**
The problems could be:
1. The inputs, outputs can be different lengths in different examples.
2. The standard NN does not share features learned across different positions of text.


# Forward Propagation of an RNN  
![fp](/assets/images/NLP/003/rnn-fp.png)



# References
- Deep Learning with Python by Francois Chollet, Chapter 6