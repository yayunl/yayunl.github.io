---
layout: post
title:  "Self-attention"
date:   2024-08-13 09:00:00
categories: NLP deep-learning
tags: self-attention
excerpt: This post discusses the self-attention algorithm.
mathjax: true
use_mermaid: true
---

* content
{:toc}

# Self-attention Explained

![self-attention](/assets/images/sequence_model/001/self-attention-andrew.png)

![self-attention2](/assets/images/sequence_model/001/self-attention.png)

# Multi-headed Attention Explained
Please check out my write-up about the topic at 
[Self-attention-and-multi-headed-attention.pdf](https://drive.google.com/file/d/1PB0-IObieawTzsgNr0xFiUeixbmUkd9n/view?usp=sharing)

# References
- Self-attention (1) by Hung-yi Lee [Lecture](https://www.youtube.com/watch?v=hYdO9CscNes)
- DLS Course 5 by Andrew Ng
- Natural Language Processing with Transformers by Lewis Tunstall, Leandro Werra, and Thomas Wolf
